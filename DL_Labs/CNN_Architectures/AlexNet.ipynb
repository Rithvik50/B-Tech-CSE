{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "U1kN3dG7yQ6E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetSmall(nn.Module):\n",
        "    def __init__(self, input_channels=3, num_classes=10, input_size=32):\n",
        "        super(AlexNetSmall, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # Dynamically compute the feature map size\n",
        "        dummy_input = torch.randn(1, input_channels, input_size, input_size)\n",
        "        feature_map_size = self.features(dummy_input).view(1, -1).size(1)\n",
        "\n",
        "        # Define the classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(feature_map_size, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten feature maps\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "lQOSQrYoyZqX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "transform_mnist = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "transform_cifar10 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, transform=transform_mnist, download=True)\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, transform=transform_mnist, download=True)\n",
        "\n",
        "cifar10_train = datasets.CIFAR10(root='./data', train=True, transform=transform_cifar10, download=True)\n",
        "cifar10_test = datasets.CIFAR10(root='./data', train=False, transform=transform_cifar10, download=True)\n",
        "\n",
        "mnist_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
        "mnist_test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
        "\n",
        "cifar10_loader = DataLoader(cifar10_train, batch_size=64, shuffle=True)\n",
        "cifar10_test_loader = DataLoader(cifar10_test, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfJNCz2oyrZv",
        "outputId": "7b106faf-f28d-49f1-acba-ff0da2ee8e4a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, dataloader, criterion, optimizer, epochs=10, device='cuda'):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(dataloader)}\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader, device='cuda'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n"
      ],
      "metadata": {
        "id": "25R45l2ayu6b"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate models for MNIST and CIFAR-10\n",
        "model_mnist = AlexNetSmall(input_channels=1, num_classes=10, input_size=28)  # MNIST: single-channel, 28x28\n",
        "model_cifar10 = AlexNetSmall(input_channels=3, num_classes=10, input_size=32)  # CIFAR-10: RGB, 32x32\n",
        "\n",
        "# Loss function and optimizers\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_mnist = optim.Adam(model_mnist.parameters(), lr=0.001)\n",
        "optimizer_cifar10 = optim.Adam(model_cifar10.parameters(), lr=0.001)\n",
        "\n",
        "# Train models\n",
        "print(\"Training on MNIST...\")\n",
        "train_model(model_mnist, mnist_loader, criterion, optimizer_mnist, epochs=10)\n",
        "\n",
        "print(\"Training on CIFAR-10...\")\n",
        "train_model(model_cifar10, cifar10_loader, criterion, optimizer_cifar10, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xklZ4-ifzM-U",
        "outputId": "1b4db035-010d-4121-f627-c449e1285fb1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on MNIST...\n",
            "Epoch 1, Loss: 0.3120521061866582\n",
            "Epoch 2, Loss: 0.0829659236697441\n",
            "Epoch 3, Loss: 0.06493796481250928\n",
            "Epoch 4, Loss: 0.05780306165435176\n",
            "Epoch 5, Loss: 0.04678346093722305\n",
            "Epoch 6, Loss: 0.03934474174501157\n",
            "Epoch 7, Loss: 0.036584022263451345\n",
            "Epoch 8, Loss: 0.03449093912196088\n",
            "Epoch 9, Loss: 0.03415327072666233\n",
            "Epoch 10, Loss: 0.030134496451542682\n",
            "Training on CIFAR-10...\n",
            "Epoch 1, Loss: 1.5988977322797946\n",
            "Epoch 2, Loss: 1.2088941129882012\n",
            "Epoch 3, Loss: 1.0180883635492886\n",
            "Epoch 4, Loss: 0.9056638935414116\n",
            "Epoch 5, Loss: 0.8158742270963576\n",
            "Epoch 6, Loss: 0.7373091511409301\n",
            "Epoch 7, Loss: 0.6894703542484957\n",
            "Epoch 8, Loss: 0.6450552409109862\n",
            "Epoch 9, Loss: 0.6057050711549151\n",
            "Epoch 10, Loss: 0.5622022246841885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate models\n",
        "print(\"Evaluating MNIST model...\")\n",
        "mnist_accuracy = evaluate_model(model_mnist, mnist_test_loader)\n",
        "print(f\"MNIST Accuracy: {mnist_accuracy}%\")\n",
        "\n",
        "print(\"Evaluating CIFAR-10 model...\")\n",
        "cifar10_accuracy = evaluate_model(model_cifar10, cifar10_test_loader)\n",
        "print(f\"CIFAR-10 Accuracy: {cifar10_accuracy}%\")\n"
      ],
      "metadata": {
        "id": "fVMlzdyWzXxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ca057b-f2ad-4477-e8da-8372ba53a30b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating MNIST model...\n",
            "MNIST Accuracy: 98.8%\n",
            "Evaluating CIFAR-10 model...\n",
            "CIFAR-10 Accuracy: 75.63%\n"
          ]
        }
      ]
    }
  ]
}